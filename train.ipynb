{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BigBigora\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\BigBigora\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000 00:03 loss: 9.62743091583252 acc: 0.0\n",
      "00000100 00:33 loss: 7.061116695404053 acc: 0.0989096537232399\n",
      "00000200 01:04 loss: 6.839704990386963 acc: 0.11538461595773697\n",
      "00000300 01:35 loss: 6.816007137298584 acc: 0.12024825811386108\n",
      "00000400 02:07 loss: 6.536980152130127 acc: 0.143968865275383\n",
      "00000500 02:38 loss: 6.213496208190918 acc: 0.16900311410427094\n",
      "00000600 03:10 loss: 6.049178600311279 acc: 0.18562401831150055\n",
      "00000700 03:42 loss: 5.697995662689209 acc: 0.20108695328235626\n",
      "00000800 04:15 loss: 5.7117600440979 acc: 0.18230357766151428\n",
      "00000900 04:48 loss: 5.310678005218506 acc: 0.21101629734039307\n",
      "00001000 05:21 loss: 5.2506256103515625 acc: 0.22102008759975433\n",
      "00001100 05:54 loss: 5.166344165802002 acc: 0.2167721539735794\n",
      "00001200 06:27 loss: 4.845065593719482 acc: 0.26514554023742676\n",
      "00001300 07:01 loss: 4.974267482757568 acc: 0.23222748935222626\n",
      "00001400 07:34 loss: 4.77531099319458 acc: 0.2562893033027649\n",
      "00001500 08:08 loss: 4.630521297454834 acc: 0.26299455761909485\n",
      "00001600 08:42 loss: 4.374740123748779 acc: 0.28105589747428894\n",
      "00001700 09:16 loss: 4.02148962020874 acc: 0.33359014987945557\n",
      "00001800 09:49 loss: 4.042994022369385 acc: 0.32538461685180664\n",
      "00001900 10:23 loss: 3.933765411376953 acc: 0.32940250635147095\n",
      "00002000 10:57 loss: 4.158888816833496 acc: 0.3067193627357483\n",
      "00002100 11:31 loss: 4.126847743988037 acc: 0.301499605178833\n",
      "00002200 12:05 loss: 3.7906670570373535 acc: 0.33565351366996765\n",
      "00002300 12:38 loss: 3.6670196056365967 acc: 0.352351576089859\n",
      "00002400 13:14 loss: 3.3811142444610596 acc: 0.3785880506038666\n",
      "00002500 13:48 loss: 3.5070509910583496 acc: 0.3628593683242798\n",
      "00002600 14:22 loss: 3.322519063949585 acc: 0.38205721974372864\n",
      "00002700 14:56 loss: 3.3172078132629395 acc: 0.3779342770576477\n",
      "00002800 15:28 loss: 2.845874786376953 acc: 0.44426876306533813\n",
      "00002900 16:01 loss: 2.8032360076904297 acc: 0.427139550447464\n",
      "00003000 16:35 loss: 2.65592360496521 acc: 0.45852896571159363\n",
      "00003100 17:08 loss: 2.688192129135132 acc: 0.4523625075817108\n",
      "00003200 17:42 loss: 2.4099533557891846 acc: 0.5165511965751648\n",
      "00003300 18:15 loss: 2.4025003910064697 acc: 0.49723756313323975\n",
      "00003400 18:48 loss: 2.163151741027832 acc: 0.5558194518089294\n",
      "00003500 19:21 loss: 2.13504958152771 acc: 0.5650117993354797\n",
      "00003600 19:55 loss: 2.224107027053833 acc: 0.5343213677406311\n",
      "00003700 20:29 loss: 2.1937925815582275 acc: 0.5533905029296875\n",
      "00003800 21:02 loss: 1.8947699069976807 acc: 0.5957943797111511\n",
      "00003900 21:35 loss: 1.8231700658798218 acc: 0.6290571689605713\n",
      "00004000 22:08 loss: 1.68800950050354 acc: 0.6405750513076782\n",
      "00004100 22:42 loss: 1.6119401454925537 acc: 0.6449332237243652\n",
      "00004200 23:15 loss: 1.4287521839141846 acc: 0.6849420666694641\n",
      "00004300 23:48 loss: 1.548370599746704 acc: 0.6741213798522949\n",
      "00004400 24:21 loss: 1.365854024887085 acc: 0.7096773982048035\n",
      "00004500 24:54 loss: 1.2952474355697632 acc: 0.7128325700759888\n",
      "00004600 25:27 loss: 1.1876335144042969 acc: 0.7402191162109375\n",
      "00004700 26:01 loss: 1.098067045211792 acc: 0.7655763030052185\n",
      "00004800 26:35 loss: 1.090303659439087 acc: 0.7642912864685059\n",
      "00004900 27:08 loss: 0.9218627214431763 acc: 0.8060278296470642\n",
      "00005000 27:42 loss: 0.8994752764701843 acc: 0.8118279576301575\n",
      "00005100 28:14 loss: 0.8007802367210388 acc: 0.8271889686584473\n",
      "00005200 28:48 loss: 0.7236170172691345 acc: 0.8465080857276917\n",
      "00005300 29:21 loss: 0.7472143769264221 acc: 0.8332040309906006\n",
      "00005400 29:54 loss: 0.787189245223999 acc: 0.8271799087524414\n",
      "00005500 30:27 loss: 0.7022643685340881 acc: 0.8542319536209106\n",
      "00005600 31:01 loss: 0.6860964894294739 acc: 0.8515384793281555\n",
      "00005700 31:34 loss: 0.5988554954528809 acc: 0.8792423009872437\n",
      "00005800 32:06 loss: 0.591105043888092 acc: 0.8781446814537048\n",
      "00005900 32:39 loss: 0.5881345868110657 acc: 0.8605918884277344\n",
      "00006000 33:12 loss: 0.4579213857650757 acc: 0.9158222675323486\n",
      "00006100 33:45 loss: 0.46798521280288696 acc: 0.9006928205490112\n",
      "00006200 34:18 loss: 0.4140464961528778 acc: 0.9130775332450867\n",
      "00006300 34:51 loss: 0.4330447316169739 acc: 0.9127358198165894\n",
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Data_load import (get_batch_indices, load_cn_vocab,\n",
    "                                        load_en_vocab, load_train_data,\n",
    "                                        maxlen)\n",
    "from model import Transformer\n",
    "\n",
    "# Config\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "heads = 8\n",
    "dropout_rate = 0.2\n",
    "n_epochs = 60\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda'\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    # X: en\n",
    "    # Y: cn\n",
    "    Y, X = load_train_data()\n",
    "\n",
    "    print_interval = 100\n",
    "\n",
    "    model = Transformer(len(en2idx), len(cn2idx), PAD_ID, d_model, d_ff,\n",
    "                        n_layers, heads, dropout_rate, maxlen)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    citerion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "    tic = time.time()\n",
    "    cnter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for index, _ in get_batch_indices(len(X), batch_size):\n",
    "            x_batch = torch.LongTensor(X[index]).to(device)\n",
    "            y_batch = torch.LongTensor(Y[index]).to(device)\n",
    "            y_input = y_batch[:, :-1]\n",
    "            y_label = y_batch[:, 1:]\n",
    "            y_hat = model(x_batch, y_input)\n",
    "\n",
    "            y_label_mask = y_label != PAD_ID\n",
    "            preds = torch.argmax(y_hat, -1)\n",
    "            correct = preds == y_label\n",
    "            acc = torch.sum(y_label_mask * correct) / torch.sum(y_label_mask)\n",
    "\n",
    "            n, seq_len = y_label.shape\n",
    "            y_hat = torch.reshape(y_hat, (n * seq_len, -1))\n",
    "            y_label = torch.reshape(y_label, (n * seq_len, ))\n",
    "            loss = citerion(y_hat, y_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if cnter % print_interval == 0:\n",
    "                toc = time.time()\n",
    "                interval = toc - tic\n",
    "                minutes = int(interval // 60)\n",
    "                seconds = int(interval % 60)\n",
    "                print(f'{cnter:08d} {minutes:02d}:{seconds:02d}'\n",
    "                    f' loss: {loss.item()} acc: {acc.item()}')\n",
    "            cnter += 1\n",
    "\n",
    "    model_path = 'model.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
