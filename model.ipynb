{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BigBigora\\anaconda3\\envs\\pytorch1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tst = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n",
    "print(tst)\n",
    "NormLayer = nn.LayerNorm(3)\n",
    "print(NormLayer(tst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        \n",
    "        assert d_model % 2 == 0\n",
    "        \n",
    "        i_seq = torch.linspace(0, max_seq_len - 1, max_seq_len)\n",
    "        j_seq = torch.linspace(0, d_model - 2, d_model // 2)\n",
    "        \n",
    "        pos, two_i = torch.meshgrid(i_seq, j_seq)\n",
    "        pe_2i = torch.sin(pos/ 10000 ** (2 * two_i / d_model))\n",
    "        pe_2i_1 = torch.cos(pos/ 10000 ** (2 * two_i / d_model))\n",
    "        pe = torch.stack((pe_2i, pe_2i_1), 2).reshape(1, max_seq_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe, False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        n, seq_len, d_model = x.shape\n",
    "        pe: torch.Tensor = self.pe\n",
    "        assert seq_len <= pe.shape[1]\n",
    "        assert d_model == pe.shape[2]\n",
    "        rescaled_x = x * d_model**0.5\n",
    "        return rescaled_x + pe[:, 0:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_INF = 1e12\n",
    "\n",
    "def attention(q: torch.Tensor, k:torch.Tensor, v:torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "    \n",
    "    assert q.shape[-1] == k.shape[-1]\n",
    "    d_k = k.shape[-1]\n",
    "    tmp = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5\n",
    "    \n",
    "    if mask is not None:\n",
    "        tmp.masked_fill_(mask, -MY_INF)\n",
    "    \n",
    "    tmp = F.softmax(tmp, -1)\n",
    "    tmp = torch.matmul(tmp, v)\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentiion(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        \n",
    "        self.d_k = d_model // heads #d_k the dimension of each head\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.q = nn.Linear(d_model, d_model) #linear transformation for Q, K, V\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        assert q.shape[0] == k.shape[0] #assert the batch size of Q and K are same\n",
    "        assert q.shape[0] == v.shape[0] #assert the batch size of Q and V are same\n",
    "        assert k.shape[1] == v.shape[1] #assert the Sequence_length  of K and V are same\n",
    "        \n",
    "        n, q_len = q.shape[0:2]\n",
    "        n, k_len = k.shape[0:2]\n",
    "        \n",
    "        q_ = self.q(q).reshape(n, q_len, self.heads, self.d_k).transpose(1, 2) #reform the Q after the linear transformation. The reformation trans Q(n, q_len, d_model) to q_(n, q_len, self.heads, self.d_k). transpose(1, 2) reform Q to (n, self.heads, q_len, self.d_k)\n",
    "        k_ = self.k(k).reshape(n, k_len, self.heads, self.d_k).transpose(1, 2)\n",
    "        v_ = self.v(v).reshape(n, k_len, self.heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attention_res = attention(q_, k_, v_, mask)\n",
    "        concat_res = attention_res.transpose(1, 2).reshape(n, q_len, self.d_model)\n",
    "        concat_res = self.dropout(concat_res)\n",
    "        \n",
    "        output = self.out(concat_res)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_ff) # d_ff is lager than d_model\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentiion(heads, d_model, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
    "        tmp = self.self_attention(x, x, x, src_mask)\n",
    "        tmp = self.dropout1(tmp)\n",
    "        x = self.norm1(x + tmp)\n",
    "        tmp = self.ffn(x)\n",
    "        tmp = self.dropout2(tmp)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads: int, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttentiion(heads, d_model, dropout)\n",
    "        self.attention = MultiHeadAttentiion(heads, d_model, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_kv: torch.Tensor, src_dst_mask: Optional[torch.Tensor] = None):\n",
    "        tmp = self.self_attention(x, x, x, src_dst_mask)\n",
    "        tmp = self.dropout1(tmp)\n",
    "        x = self.norm1(x + tmp)\n",
    "        tmp = self.attention(x, encoder_kv, encoder_kv, src_dst_mask)\n",
    "        tmp = self.dropout2(tmp)\n",
    "        x = self.norm2(x + tmp)\n",
    "        tmp = self.ffn(x)\n",
    "        tmp = self.dropout3(tmp)\n",
    "        x = self.norm3(x + tmp)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15480\\1677753629.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     def __init__(self,\n\u001b[0;32m      4\u001b[0m                 \u001b[0mvocab_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mpad_idx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                vocab_size: int,\n",
    "                pad_idx: int,\n",
    "                d_model: int,\n",
    "                d_ff: int,\n",
    "                n_layers: int,\n",
    "                heads: int,\n",
    "                dropout: float = 0.1,\n",
    "                max_seq_len: int = 120):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(EncoderLayer(heads, d_model, d_ff, dropout))\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                vocab_size: int,\n",
    "                pad_idx: int,\n",
    "                d_model: int,\n",
    "                d_ff: int,\n",
    "                n_layers: int,\n",
    "                heads: int,\n",
    "                dropout: float = 0.1,\n",
    "                max_seq_len: int = 120):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)\n",
    "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(DecoderLayer(heads, d_model, d_ff, dropout))\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x,\n",
    "                encoder_kv,\n",
    "                dst_mask: Optional[torch.Tensor] = None,\n",
    "                src_dst_mask: Optional[torch.Tensor] = None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_kv, dst_mask, src_dst_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_vocab_size: int, dst_vocab_size: int, pad_idx:int, d_model: int, d_ff: int, n_layers: int, heads: int, dropout: float = 0.1, max_seq_len: int = 200):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, pad_idx, d_model, d_ff, n_layers, heads, dropout, max_seq_len)\n",
    "        self.decoder = Decoder(dst_vocab_size, pad_idx, d_model, d_ff, n_layers, heads, dropout, max_seq_len)\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.output_layer = nn.Linear(d_model, dst_vocab_size)\n",
    "        \n",
    "    def generate_mask(self,\n",
    "                    q_pad: torch.Tensor,\n",
    "                    k_pad: torch.Tensor,\n",
    "                    with_left_mask: bool = False):\n",
    "        # q_pad shape: [n, q_len]\n",
    "        # k_pad shape: [n, k_len]\n",
    "        # q_pad k_pad dtype: bool\n",
    "        assert q_pad.device == k_pad.device\n",
    "        n, q_len = q_pad.shape\n",
    "        n, k_len = k_pad.shape\n",
    "\n",
    "        mask_shape = (n, 1, q_len, k_len)\n",
    "        if with_left_mask:\n",
    "            mask = 1 - torch.tril(torch.ones(mask_shape))\n",
    "        else:\n",
    "            mask = torch.zeros(mask_shape)\n",
    "        mask = mask.to(q_pad.device)\n",
    "        for i in range(n):\n",
    "            mask[i, :, q_pad[i], :] = 1\n",
    "            mask[i, :, :, k_pad[i]] = 1\n",
    "        mask = mask.to(torch.bool)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        src_pad_mask = x == self.pad_idx\n",
    "        dst_pad_mask = y == self.pad_idx\n",
    "        src_mask = self.generate_mask(src_pad_mask, src_pad_mask, False)\n",
    "        dst_mask = self.generate_mask(dst_pad_mask, dst_pad_mask, True)\n",
    "        src_dst_mask = self.generate_mask(dst_pad_mask, src_pad_mask, False)\n",
    "        encoder_kv = self.encoder(x, src_mask)\n",
    "        res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)\n",
    "        res = self.output_layer(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
