{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import random\n",
    "\n",
    "import regex\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex库用来处理正则表达式：匹配，搜索，替换，拆分\n",
    "request库用来发送HTTP请求，实现网络通讯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cnt = 0\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = '../Data/cn.txt'\n",
    "target_train = '../Data/en.txt'\n",
    "source_test = '../Data/cn.test.txt'\n",
    "target_test = '../Data/en.test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_cnt为单词最小出现的频率，如果出现次数小于min_cnt则将单词encode为<UNK>\n",
    "\n",
    "maxlen为一句话中最多的单词数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(language):\n",
    "    \n",
    "    assert language in ['cn', 'en'], 'Language should be Chinese or English'\n",
    "    vocab = [\n",
    "        line.split()[0] for line in codecs.open(\n",
    "            '../Data/{}.txt.vocab.tsv'.format(language),\n",
    "            'r',\n",
    "            'utf-8'\n",
    "        ).read().splitlines if int(line.split()[1]) >= min_cnt\n",
    "    ]\n",
    "    \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codecs.open()以规定形式打开文件\n",
    "read()方法读取文件所有内容\n",
    "splitline()方法将所有内容按行分割，生成一个包含每行内容的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cn_vocab():\n",
    "    word2idx, idx2word = load_vocab('cn')\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def load_en_vocab():\n",
    "    word2idx, idx2word = load_vocab('en')\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(source_sents, target_sents):\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "\n",
    "    # Index\n",
    "    x_list, y_list, Sources, Targets = [], [], [], []\n",
    "    for source_sent, target_sent in zip(source_sents, target_sents):\n",
    "        x = [\n",
    "            cn2idx.get(word, 1)\n",
    "            for word in ('<S> ' + source_sent + ' </S>').split()\n",
    "        ]  # 1: OOV, </S>: End of Text\n",
    "        y = [\n",
    "            en2idx.get(word, 1)\n",
    "            for word in ('<S> ' + target_sent + ' </S>').split()\n",
    "        ]\n",
    "        if max(len(x), len(y)) <= maxlen:\n",
    "            x_list.append(np.array(x))\n",
    "            y_list.append(np.array(y))\n",
    "            Sources.append(source_sent)\n",
    "            Targets.append(target_sent)\n",
    "\n",
    "    # Pad\n",
    "    X = np.zeros([len(x_list), maxlen], np.int32)\n",
    "    Y = np.zeros([len(y_list), maxlen], np.int32)\n",
    "    for i, (x, y) in enumerate(zip(x_list, y_list)):\n",
    "        X[i] = np.lib.pad(x, [0, maxlen - len(x)],\n",
    "                        'constant',\n",
    "                        constant_values=(0, 0))\n",
    "        Y[i] = np.lib.pad(y, [0, maxlen - len(y)],\n",
    "                        'constant',\n",
    "                        constant_values=(0, 0))\n",
    "    return X, Y, Sources, Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type):\n",
    "    if data_type == 'train':\n",
    "        source, target = source_train, target_train\n",
    "    elif data_type == 'test':\n",
    "        source, target = source_test, target_test\n",
    "    assert data_type in ['train', 'test']\n",
    "    cn_sents = [\n",
    "        regex.sub(\"[^\\s\\p{L}']\", '', line)  # noqa W605\n",
    "        for line in codecs.open(source, 'r', 'utf-8').read().split('\\n')\n",
    "        if line and line[0] != '<'\n",
    "    ]\n",
    "    en_sents = [\n",
    "        regex.sub(\"[^\\s\\p{L}']\", '', line)  # noqa W605\n",
    "        for line in codecs.open(target, 'r', 'utf-8').read().split('\\n')\n",
    "        if line and line[0] != '<'\n",
    "    ]\n",
    "\n",
    "    X, Y, Sources, Targets = create_data(cn_sents, en_sents)\n",
    "    return X, Y, Sources, Targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    X, Y, _, _ = load_data('train')\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    X, Y, _, _ = load_data('test')\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_batch_indices(total_length, batch_size):\n",
    "    assert (batch_size <=\n",
    "            total_length), ('Batch size is large than total data length.'\n",
    "                            'Check your data or change batch size.')\n",
    "    current_index = 0\n",
    "    indexs = [i for i in range(total_length)]\n",
    "    random.shuffle(indexs)\n",
    "    while 1:\n",
    "        if current_index + batch_size >= total_length:\n",
    "            break\n",
    "        current_index += batch_size\n",
    "        yield indexs[current_index:current_index + batch_size], current_index\n",
    "\n",
    "\n",
    "def idx_to_sentence(arr, vocab, insert_space=False):\n",
    "    res = ''\n",
    "    first_word = True\n",
    "    for id in arr:\n",
    "        word = vocab[id.item()]\n",
    "\n",
    "        if insert_space and not first_word:\n",
    "            res += ' '\n",
    "        first_word = False\n",
    "\n",
    "        res += word\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, dir, name=None):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    if name is None:\n",
    "        name = url.split('/')[-1]\n",
    "    path = os.path.join(dir, name)\n",
    "    if not os.path.exists(path):\n",
    "        print(f'Install {name} ...')\n",
    "        open(path, 'wb').write(requests.get(url).content)\n",
    "        print('Install successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    data_dir = 'dldemos/Transformer/data'\n",
    "    urls = [('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "            'master/corpora/cn.txt'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "            'master/corpora/en.txt'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "            'master/preprocessed/cn.txt.vocab.tsv'),\n",
    "            ('https://raw.githubusercontent.com/P3n9W31/transformer-pytorch/'\n",
    "            'master/preprocessed/en.txt.vocab.tsv')]\n",
    "    for url in urls:\n",
    "        download(url, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install cn.txt ...\n",
      "Install successfully.\n",
      "Install en.txt ...\n",
      "Install successfully.\n",
      "Install cn.txt.vocab.tsv ...\n",
      "Install successfully.\n",
      "Install en.txt.vocab.tsv ...\n",
      "Install successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    download_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
