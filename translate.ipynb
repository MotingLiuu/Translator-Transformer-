{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the broader the scale of unity and the more the people are united the greater the certainty for our party in overcoming difficulties and winning victories\n",
      "<S> 团结 的 面越 宽 团结 的 人 越 多 我们 党 战胜 困难 夺取 胜利 的 把握 就 越 大 团结 越 大 </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from Data_load import (idx_to_sentence, load_cn_vocab,\n",
    "                                        load_en_vocab, maxlen)\n",
    "from model import Transformer\n",
    "\n",
    "# Config\n",
    "batch_size = 1\n",
    "lr = 0.0001\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "heads = 8\n",
    "dropout_rate = 0.2\n",
    "n_epochs = 60\n",
    "\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda'\n",
    "    cn2idx, idx2cn = load_cn_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "\n",
    "    model = Transformer(len(en2idx), len(cn2idx), 0, d_model, d_ff, n_layers,\n",
    "                        heads, dropout_rate, maxlen)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_path = 'model.pth'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    my_input = 'the broader the scale of unity and the more the people are united the greater the certainty for our party in overcoming difficulties and winning victories'.split()\n",
    "    x_batch = torch.LongTensor([[en2idx[x] for x in my_input]]).to(device)\n",
    "\n",
    "    cn_sentence = idx_to_sentence(x_batch[0], idx2en, True)\n",
    "    print(cn_sentence)\n",
    "\n",
    "    y_input = torch.ones(batch_size, maxlen,\n",
    "                         dtype=torch.long).to(device) * PAD_ID\n",
    "    y_input[0] = en2idx['<S>']\n",
    "    # y_input = y_batch\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, y_input.shape[1]):\n",
    "            y_hat = model(x_batch, y_input)\n",
    "            for j in range(batch_size):\n",
    "                y_input[j, i] = torch.argmax(y_hat[j, i - 1])\n",
    "    output_sentence = idx_to_sentence(y_input[0], idx2cn, True)\n",
    "    print(output_sentence)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
